{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5481c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\college\\btech_sem_2\\carbonscrope-main\\.conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers !pip install protobuf==3.20.3 rank_bm25\n",
    "# !pip install -U langchain-huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76eedb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9992\\2093791242.py:41: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = hybrid_retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hybrid Retrieval Results ===\n",
      "\n",
      "Result 1:\n",
      "Energy_Type: i | Energy_Lang: Ruby | Energy_Value: 69.91 | Time_Type: i | Time_Lang: Perl | Time_Value: 65.79 | Mb_Type: v | Mb_Lang: Erlang | Mb_Value: 7.2\n",
      "\n",
      "Result 2:\n",
      "Energy_Type: c | Energy_Lang: C | Energy_Value: 1.0 | Time_Type: c | Time_Lang: C | Time_Value: 1.0 | Mb_Type: c | Mb_Lang: Pascal | Mb_Value: 1.0\n",
      "\n",
      "Result 3:\n",
      "Energy_Type: c | Energy_Lang: C | Energy_Value: 1.0 | Time_Type: c | Time_Lang: C | Time_Value: 1.0 | Mb_Type: c | Mb_Lang: Pascal | Mb_Value: 1.0 | text: Energy_Type: c | Energy_Lang: C | Energy_Value: 1.0 | Time_Type: c | Time_Lang: C | Time_Value: 1.0 | Mb_Type: c | Mb_Lang: Pascal | Mb_Value: 1.0\n",
      "\n",
      "Result 4:\n",
      "Energy_Type: c | Energy_Lang: C++ | Energy_Value: 1.34 | Time_Type: c | Time_Lang: C++ | Time_Value: 1.56 | Mb_Type: c | Mb_Lang: C | Mb_Value: 1.17 | text: Energy_Type: c | Energy_Lang: C++ | Energy_Value: 1.34 | Time_Type: c | Time_Lang: C++ | Time_Value: 1.56 | Mb_Type: c | Mb_Lang: C | Mb_Value: 1.17\n",
      "\n",
      "Result 5:\n",
      "Energy_Type: c | Energy_Lang: Haskell | Energy_Value: 3.1 | Time_Type: c | Time_Lang: Haskell | Time_Value: 3.55 | Mb_Type: i | Mb_Lang: Python | Mb_Value: 2.8\n",
      "\n",
      "Result 6:\n",
      "Energy_Type: v | Energy_Lang: C# | Energy_Value: 3.14 | Time_Type: c | Time_Lang: Swift | Time_Value: 4.2 | Mb_Type: c | Mb_Lang: Ocaml | Mb_Value: 2.82\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Required imports\n",
    "from pinecone import Pinecone\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
    "\n",
    "# üîê Pinecone config\n",
    "api_key    = \"pcsk_7Mazgv_HDzp35WajyQ8yot7gB5NVaPJwAa3vCNxoSCJWBqsifUS32pukGGaMR5vQjVWts1\"\n",
    "index_name = \"carbonscope1\"\n",
    "region     = \"us-east-1\"\n",
    "\n",
    "# üîå Connect to Pinecone index\n",
    "pc = Pinecone(api_key=api_key)\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "# üìê Embedding & vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddings,\n",
    "    text_key=\"text\",\n",
    "    namespace=\"default\",\n",
    "    distance_strategy=\"COSINE\",\n",
    "    pinecone_api_key=api_key,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# üîç Build sparse + dense + hybrid retrievers\n",
    "# Note: Ensure the text corpus used in BM25Retriever is the same as upserted in Pinecone\n",
    "all_texts = [doc.page_content for doc in vector_store.similarity_search(\"sample\", k=100)]\n",
    "\n",
    "sparse_retriever = BM25Retriever.from_texts(all_texts)\n",
    "dense_retriever  = vector_store.as_retriever()\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weights=[0.6, 0.4]\n",
    ")\n",
    "\n",
    "# üß™ Example retrieval\n",
    "query   = \"Which language is most energy efficient?\"\n",
    "results = hybrid_retriever.get_relevant_documents(query)\n",
    "\n",
    "# üì§ Output\n",
    "print(\"\\n=== Hybrid Retrieval Results ===\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\\n{doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa8222d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gemini Response ===\n",
      "\n",
      "Based on the provided data, C is the most energy-efficient language. It has an Energy_Value of 1.0, which is the lowest among the listed languages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 9Ô∏è‚É£ Gemini API call to answer from retrieved data\n",
    "import google.generativeai as genai\n",
    "# üí° Replace with your actual Gemini API Key:\n",
    "genai.configure(api_key=\"AIzaSyDPHpkLVLkaJS4ro24wwMvu-fBadvqdxBw\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "context = \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "gemini_prompt = f\"\"\"\n",
    "You are an expert assistant. Based on the following data, answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Give a precise and data-backed response.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(gemini_prompt)\n",
    "\n",
    "print(\"\\n=== Gemini Response ===\\n\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01285438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [21796]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "INFO:     Waiting for application shutdown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ FastAPI Server with Pinecone + LangChain + Gemini, Runnable in Jupyter Notebook\n",
    "\n",
    "# üì¶ Required imports\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
    "import google.generativeai as genai\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "\n",
    "# üîê API Keys & Config\n",
    "PINECONE_API_KEY = \"pcsk_7Mazgv_HDzp35WajyQ8yot7gB5NVaPJwAa3vCNxoSCJWBqsifUS32pukGGaMR5vQjVWts1\"\n",
    "PINECONE_INDEX = \"carbonscope1\"\n",
    "PINECONE_REGION = \"us-east-1\"\n",
    "GEMINI_API_KEY = \"AIzaSyDPHpkLVLkaJS4ro24wwMvu-fBadvqdxBw\"\n",
    "\n",
    "# üì¶ FastAPI Setup\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],  # Or [\"*\"] for development\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "# üì® Input Schema\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    projectId: str\n",
    "\n",
    "\n",
    "# üîå Pinecone Connection & Vector Store Setup\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pinecone_index = pc.Index(PINECONE_INDEX)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddings,\n",
    "    text_key=\"text\",\n",
    "    namespace=\"default\",\n",
    "    distance_strategy=\"COSINE\",\n",
    "    pinecone_api_key=PINECONE_API_KEY,\n",
    "    index_name=PINECONE_INDEX\n",
    ")\n",
    "\n",
    "# üß† Gemini Setup\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# üîç Hybrid Retriever\n",
    "def build_hybrid_retriever(sample_query=\"sample\"):\n",
    "    all_texts = [doc.page_content for doc in vector_store.similarity_search(sample_query, k=100)]\n",
    "    sparse_retriever = BM25Retriever.from_texts(all_texts)\n",
    "    dense_retriever = vector_store.as_retriever()\n",
    "    hybrid_retriever = EnsembleRetriever(\n",
    "        retrievers=[dense_retriever, sparse_retriever],\n",
    "        weights=[0.6, 0.4]\n",
    "    )\n",
    "    return hybrid_retriever\n",
    "\n",
    "# üîÅ Endpoint\n",
    "@app.post(\"/query\")\n",
    "def handle_query(request: QueryRequest):\n",
    "    query = request.query\n",
    "    projectId = request.projectId\n",
    "\n",
    "    # --- Step 1: Try Hybrid RAG Retrieval ---\n",
    "    retriever = build_hybrid_retriever()\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "    # --- Step 2: Get All Project Data ---\n",
    "    try:\n",
    "        response = requests.get(f\"http://localhost:3000/api/projectById?projectId={projectId}\")\n",
    "        print(response.text)\n",
    "        project_data = response.json()\n",
    "    except Exception as e:\n",
    "        project_data = []\n",
    "        print(\"Failed to fetch projects:\", e)\n",
    "\n",
    "    # --- Step 3: Format Project Insights (optional: trim or condense) ---\n",
    "    project_summary = f\"\"\"\n",
    "    - Project: {project_data.get('name', 'Unnamed')}\n",
    "      Domain: {project_data.get('domain', 'Unknown')}\n",
    "      Languages: {json.dumps(project_data.get('languages_used', {}))}\n",
    "      Suggestions: {json.dumps(project_data.get('suggestions', {}))}\n",
    "      Energy: {json.dumps(project_data.get('energy', {}))}\n",
    "      Time: {json.dumps(project_data.get('time', {}))}\n",
    "      Memory: {json.dumps(project_data.get('memory', {}))}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # --- Step 4: Construct Prompt ---\n",
    "    gemini_prompt = f\"\"\"\n",
    "You are an expert AI advisor helping software developers pick the most sustainable and efficient programming languages.\n",
    "\n",
    "--- Context from RAG ---\n",
    "{context if context.strip() else \"No useful context retrieved.\"}\n",
    "\n",
    "--- Project Data ---\n",
    "{project_summary}\n",
    "\n",
    "--- User Question ---\n",
    "{query}\n",
    "\n",
    "Instructions:\n",
    "- Use the context and project data above if helpful.\n",
    "- If they are not useful or incomplete, rely on your own expert knowledge.\n",
    "- Recommend the best language based on energy, time, memory, and domain.\n",
    "- Suggest better alternatives where appropriate.\n",
    "- Be clear, practical, and eco-conscious.\n",
    "\"\"\"\n",
    "\n",
    "    # --- Step 5: Get Answer from Gemini ---\n",
    "    response = gemini_model.generate_content(gemini_prompt)\n",
    "    return {\"answer\": response.text}\n",
    "\n",
    "\n",
    "# üîÑ Start Server in Background Thread\n",
    "def run_app():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "server_thread = threading.Thread(target=run_app, daemon=True)\n",
    "server_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5419d2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response1 = \"\"\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba30c4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided data, C is the most energy-efficient language, with an energy value of 1.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.post(\"http://localhost:8000/query\", json={\"query\": \"Which language is most energy efficient?\"})\n",
    "print(res.json()[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
